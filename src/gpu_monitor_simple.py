import torch
import gc
import sys

def quick_gpu_check(tag=""):
    """å¿«é€Ÿæ£€æŸ¥GPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µ"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3  # è½¬æ¢ä¸ºGB
        reserved = torch.cuda.memory_reserved() / 1024**3
        print(f"ğŸ” [{tag}] GPUæ˜¾å­˜ - å·²åˆ†é…: {allocated:.2f}GB, å·²ä¿ç•™: {reserved:.2f}GB")
    else:
        print(f"[{tag}] CUDAä¸å¯ç”¨")

def check_variable(var, name):
    """æ£€æŸ¥å•ä¸ªå˜é‡çš„æ˜¾å­˜å ç”¨"""
    if isinstance(var, torch.Tensor):
        if var.is_cuda:
            size_gb = var.element_size() * var.numel() / 1024**3
            print(f"ğŸ“¦ {name}: {size_gb:.3f}GB, å½¢çŠ¶: {var.shape}, è®¾å¤‡: {var.device}")
        else:
            print(f"ğŸ“¦ {name}: CPUå¼ é‡, å½¢çŠ¶: {var.shape}")
    else:
        print(f"ğŸ“¦ {name}: ä¸æ˜¯PyTorchå¼ é‡")

def check_multiple_variables(**kwargs):
    """ä¸€æ¬¡æ£€æŸ¥å¤šä¸ªå˜é‡"""
    print("=" * 50)
    print("ğŸ“Š å˜é‡æ˜¾å­˜æ£€æŸ¥:")
    total_gpu_memory = 0
    
    for name, var in kwargs.items():
        if isinstance(var, torch.Tensor) and var.is_cuda:
            size_gb = var.element_size() * var.numel() / 1024**3
            total_gpu_memory += size_gb
            print(f"  {name}: {size_gb:.3f}GB")
        elif isinstance(var, list) and var and isinstance(var[0], torch.Tensor):
            # æ£€æŸ¥å¼ é‡åˆ—è¡¨
            list_memory = 0
            gpu_count = 0
            for tensor in var:
                if isinstance(tensor, torch.Tensor) and tensor.is_cuda:
                    list_memory += tensor.element_size() * tensor.numel() / 1024**3
                    gpu_count += 1
            if list_memory > 0:
                total_gpu_memory += list_memory
                print(f"  {name}: {list_memory:.3f}GB (åŒ…å«{gpu_count}ä¸ªGPUå¼ é‡)")
    
    print(f"  æ€»è®¡: {total_gpu_memory:.3f}GB")
    print("=" * 50)

def memory_checkpoint(checkpoint_name, show_details=False):
    """å†…å­˜æ£€æŸ¥ç‚¹ï¼Œæ˜¾ç¤ºå½“å‰çŠ¶æ€"""
    print(f"\nğŸš© æ£€æŸ¥ç‚¹: {checkpoint_name}")
    print("-" * 30)
    
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        reserved = torch.cuda.memory_reserved() / 1024**3
        max_allocated = torch.cuda.max_memory_allocated() / 1024**3
        
        print(f"å½“å‰åˆ†é…: {allocated:.3f}GB")
        print(f"å½“å‰ä¿ç•™: {reserved:.3f}GB")
        print(f"å³°å€¼åˆ†é…: {max_allocated:.3f}GB")
        
        # å¦‚æœæ˜¾å­˜ä½¿ç”¨è¿‡é«˜ï¼Œå‘å‡ºè­¦å‘Š
        device_props = torch.cuda.get_device_properties(0)
        total_memory = device_props.total_memory / 1024**3
        usage_percent = (allocated / total_memory) * 100
        
        if usage_percent > 80:
            print(f"âš ï¸ è­¦å‘Š: GPUæ˜¾å­˜ä½¿ç”¨ç‡ {usage_percent:.1f}% - å»ºè®®æ¸…ç†å†…å­˜!")
        elif usage_percent > 60:
            print(f"â„¹ï¸ æ³¨æ„: GPUæ˜¾å­˜ä½¿ç”¨ç‡ {usage_percent:.1f}%")
        else:
            print(f"âœ… æ˜¾å­˜ä½¿ç”¨æ­£å¸¸: {usage_percent:.1f}%")
    
    if show_details:
        # æ˜¾ç¤ºå½“å‰Pythonè¿›ç¨‹çš„å†…å­˜ä½¿ç”¨
        import psutil
        import os
        process = psutil.Process(os.getpid())
        ram_usage = process.memory_info().rss / 1024**3
        print(f"è¿›ç¨‹RAMä½¿ç”¨: {ram_usage:.2f}GB")
    
    print("-" * 30)

def clean_and_check(tag=""):
    """æ¸…ç†å†…å­˜å¹¶æ£€æŸ¥æ•ˆæœ"""
    print(f"ğŸ§¹ æ¸…ç†å†…å­˜ {tag}...")
    before_allocated = torch.cuda.memory_allocated() / 1024**3 if torch.cuda.is_available() else 0
    
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    after_allocated = torch.cuda.memory_allocated() / 1024**3 if torch.cuda.is_available() else 0
    freed = before_allocated - after_allocated
    
    if freed > 0:
        print(f"âœ… é‡Šæ”¾äº† {freed:.3f}GB æ˜¾å­˜")
    else:
        print("â„¹ï¸ æ²¡æœ‰é¢å¤–æ˜¾å­˜è¢«é‡Šæ”¾")
    
    quick_gpu_check(f"æ¸…ç†å{tag}")